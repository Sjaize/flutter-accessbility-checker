import pandas as pd
import json
import re
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set
import matplotlib.pyplot as plt
import seaborn as sns

class ComprehensiveDataAnalyzer:
    """ë°ì´í„°ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëª¨ë“  ê°€ëŠ¥í•œ ì •ë³´ë¥¼ í™œìš©í•˜ëŠ” ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.processed_data = None
        self.widget_captions_data = None
        self.quality_data = None
        self.analysis_results = {}
        
    def load_all_data(self):
        """ëª¨ë“  ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("=== ë°ì´í„° ë¡œë“œ ì‹œì‘ ===")
        
        # 1. ì²˜ë¦¬ëœ ì•„ì´ì½˜ ë°ì´í„°
        try:
            self.processed_data = pd.read_csv('Data/Our Data/output_MMT_icon.csv')
            print(f"âœ… ì²˜ë¦¬ëœ ì•„ì´ì½˜ ë°ì´í„°: {len(self.processed_data)} ê°œ")
        except Exception as e:
            print(f"âŒ ì²˜ë¦¬ëœ ì•„ì´ì½˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 2. ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„°
        try:
            self.widget_captions_data = pd.read_csv('Data/WC20/widget_captions.csv')
            print(f"âœ… ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„°: {len(self.widget_captions_data)} ê°œ")
        except Exception as e:
            print(f"âŒ ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # 3. í’ˆì§ˆ í‰ê°€ ë°ì´í„°
        try:
            self.quality_data = pd.read_csv('Additional materials/Alt-Text Quality (Responses)-final.csv')
            print(f"âœ… í’ˆì§ˆ í‰ê°€ ë°ì´í„°: {len(self.quality_data)} ê°œ")
        except Exception as e:
            print(f"âŒ í’ˆì§ˆ í‰ê°€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("=== ë°ì´í„° ë¡œë“œ ì™„ë£Œ ===\n")
    
    def analyze_processed_data(self):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ìƒì„¸ ë¶„ì„í•©ë‹ˆë‹¤."""
        if self.processed_data is None:
            return
        
        print("=== ì²˜ë¦¬ëœ ë°ì´í„° ìƒì„¸ ë¶„ì„ ===")
        
        # 1. ê¸°ë³¸ í†µê³„
        print(f"ì´ ë ˆì½”ë“œ: {len(self.processed_data)}")
        print(f"ì»¬ëŸ¼: {self.processed_data.columns.tolist()}")
        
        # 2. icon_context êµ¬ì¡° ë¶„ì„
        context_analysis = self._analyze_icon_context()
        
        # 3. reference_captions ë¶„ì„
        caption_analysis = self._analyze_reference_captions()
        
        # 4. generated_caption ë¶„ì„
        generated_analysis = self._analyze_generated_captions()
        
        # 5. ì•±ë³„ ë¶„ì„
        app_analysis = self._analyze_by_app()
        
        self.analysis_results['processed_data'] = {
            'context_analysis': context_analysis,
            'caption_analysis': caption_analysis,
            'generated_analysis': generated_analysis,
            'app_analysis': app_analysis
        }
        
        print("âœ… ì²˜ë¦¬ëœ ë°ì´í„° ë¶„ì„ ì™„ë£Œ\n")
    
    def _analyze_icon_context(self):
        """icon_context êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("--- icon_context êµ¬ì¡° ë¶„ì„ ---")
        
        resource_ids = []
        class_names = []
        app_names = []
        parent_classes = []
        sibling_classes = []
        
        for _, row in self.processed_data.iterrows():
            try:
                context = json.loads(row['icon_context'])
                
                # UI element info
                ui_info = context.get('UI element info', {})
                resource_id = ui_info.get('resource_id', '')
                class_name = ui_info.get('class_name', '')
                
                if resource_id:
                    resource_ids.append(resource_id)
                if class_name:
                    class_names.append(class_name)
                
                # app activity name
                app_name = context.get('app activity name', '')
                if app_name:
                    app_names.append(app_name)
                
                # parent nodes
                parent_nodes = context.get('parent node', [])
                for parent in parent_nodes:
                    parent_class = parent.get('class', '')
                    if parent_class:
                        parent_classes.append(parent_class)
                
                # sibling nodes
                sibling_nodes = context.get('sibling nodes', [])
                for sibling in sibling_nodes:
                    sibling_class = sibling.get('class', '')
                    if sibling_class:
                        sibling_classes.append(sibling_class)
                        
            except Exception as e:
                continue
        
        analysis = {
            'resource_ids': {
                'total': len(resource_ids),
                'unique': len(set(resource_ids)),
                'top_10': Counter(resource_ids).most_common(10)
            },
            'class_names': {
                'total': len(class_names),
                'unique': len(set(class_names)),
                'top_10': Counter(class_names).most_common(10)
            },
            'app_names': {
                'total': len(app_names),
                'unique': len(set(app_names)),
                'top_10': Counter(app_names).most_common(10)
            },
            'parent_classes': {
                'total': len(parent_classes),
                'unique': len(set(parent_classes)),
                'top_10': Counter(parent_classes).most_common(10)
            },
            'sibling_classes': {
                'total': len(sibling_classes),
                'unique': len(set(sibling_classes)),
                'top_10': Counter(sibling_classes).most_common(10)
            }
        }
        
        print(f"ë¦¬ì†ŒìŠ¤ ID: {analysis['resource_ids']['unique']}ê°œ ê³ ìœ ê°’")
        print(f"í´ë˜ìŠ¤ëª…: {analysis['class_names']['unique']}ê°œ ê³ ìœ ê°’")
        print(f"ì•±ëª…: {analysis['app_names']['unique']}ê°œ ê³ ìœ ê°’")
        print(f"ë¶€ëª¨ í´ë˜ìŠ¤: {analysis['parent_classes']['unique']}ê°œ ê³ ìœ ê°’")
        print(f"í˜•ì œ í´ë˜ìŠ¤: {analysis['sibling_classes']['unique']}ê°œ ê³ ìœ ê°’")
        
        return analysis
    
    def _analyze_reference_captions(self):
        """reference_captionsë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("--- reference_captions ë¶„ì„ ---")
        
        all_captions = []
        caption_lengths = []
        unique_captions = set()
        
        for _, row in self.processed_data.iterrows():
            captions = row['reference_captions'].split('|')
            all_captions.extend(captions)
            caption_lengths.append(len(captions))
            unique_captions.update(captions)
        
        # ì•¡ì…˜ ë‹¨ì–´ ë¶„ì„
        action_words = Counter()
        for caption in all_captions:
            words = caption.lower().split()
            for word in words:
                if word in ['go', 'open', 'click', 'enter', 'type', 'search', 'add', 'get', 'toggle', 'select', 'view', 'show', 'hide', 'save', 'delete', 'edit', 'share', 'like', 'favorite', 'bookmark', 'call', 'message', 'email', 'camera', 'photo', 'video', 'location', 'map', 'notification', 'settings', 'menu', 'back', 'forward', 'close', 'cancel', 'confirm', 'submit']:
                    action_words[word] += 1
        
        analysis = {
            'total_captions': len(all_captions),
            'unique_captions': len(unique_captions),
            'avg_captions_per_element': sum(caption_lengths) / len(caption_lengths),
            'caption_length_distribution': Counter(caption_lengths),
            'action_words': dict(action_words.most_common(20)),
            'top_captions': Counter(all_captions).most_common(20)
        }
        
        print(f"ì´ ìº¡ì…˜: {analysis['total_captions']}ê°œ")
        print(f"ê³ ìœ  ìº¡ì…˜: {analysis['unique_captions']}ê°œ")
        print(f"ìš”ì†Œë‹¹ í‰ê·  ìº¡ì…˜: {analysis['avg_captions_per_element']:.2f}ê°œ")
        print(f"ì•¡ì…˜ ë‹¨ì–´: {len(analysis['action_words'])}ê°œ")
        
        return analysis
    
    def _analyze_generated_captions(self):
        """generated_captionì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("--- generated_caption ë¶„ì„ ---")
        
        generated_captions = self.processed_data['generated_caption'].dropna().tolist()
        unique_generated = set(generated_captions)
        
        analysis = {
            'total_generated': len(generated_captions),
            'unique_generated': len(unique_generated),
            'top_generated': Counter(generated_captions).most_common(20)
        }
        
        print(f"ìƒì„±ëœ ìº¡ì…˜: {analysis['total_generated']}ê°œ")
        print(f"ê³ ìœ  ìƒì„± ìº¡ì…˜: {analysis['unique_generated']}ê°œ")
        
        return analysis
    
    def _analyze_by_app(self):
        """ì•±ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤."""
        print("--- ì•±ë³„ ë¶„ì„ ---")
        
        app_data = defaultdict(list)
        
        for _, row in self.processed_data.iterrows():
            try:
                context = json.loads(row['icon_context'])
                app_name = context.get('app activity name', '')
                if app_name:
                    app_data[app_name].append({
                        'resource_id': context.get('UI element info', {}).get('resource_id', ''),
                        'class_name': context.get('UI element info', {}).get('class_name', ''),
                        'captions': row['reference_captions'].split('|'),
                        'generated': row['generated_caption']
                    })
            except:
                continue
        
        analysis = {}
        for app_name, data in app_data.items():
            analysis[app_name] = {
                'total_elements': len(data),
                'unique_resource_ids': len(set([d['resource_id'] for d in data if d['resource_id']])),
                'unique_class_names': len(set([d['class_name'] for d in data if d['class_name']])),
                'total_captions': len([caption for d in data for caption in d['captions']]),
                'unique_captions': len(set([caption for d in data for caption in d['captions']]))
            }
        
        print(f"ë¶„ì„ëœ ì•±: {len(analysis)}ê°œ")
        
        return analysis
    
    def analyze_widget_captions(self):
        """ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        if self.widget_captions_data is None:
            return
        
        print("=== ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„° ë¶„ì„ ===")
        
        # 1. ê¸°ë³¸ í†µê³„
        print(f"ì´ ë ˆì½”ë“œ: {len(self.widget_captions_data)}")
        print(f"ì»¬ëŸ¼: {self.widget_captions_data.columns.tolist()}")
        
        # 2. ìº¡ì…˜ íŒ¨í„´ ë¶„ì„
        all_captions = []
        for caption in self.widget_captions_data['captions'].head(1000):  # ì„±ëŠ¥ ê³ ë ¤
            captions = caption.split('|')
            all_captions.extend(captions)
        
        # 3. ì•¡ì…˜ ë‹¨ì–´ ë¶„ì„
        action_words = Counter()
        for caption in all_captions:
            words = caption.lower().split()
            for word in words:
                if word in ['go', 'open', 'click', 'enter', 'type', 'search', 'add', 'get', 'toggle', 'select', 'view', 'show', 'hide', 'save', 'delete', 'edit', 'share', 'like', 'favorite', 'bookmark', 'call', 'message', 'email', 'camera', 'photo', 'video', 'location', 'map', 'notification', 'settings', 'menu', 'back', 'forward', 'close', 'cancel', 'confirm', 'submit']:
                    action_words[word] += 1
        
        # 4. íŒ¨í„´ ë¶„ì„
        patterns = self._extract_caption_patterns(all_captions)
        
        analysis = {
            'total_captions': len(all_captions),
            'unique_captions': len(set(all_captions)),
            'action_words': dict(action_words.most_common(20)),
            'patterns': patterns,
            'top_captions': Counter(all_captions).most_common(20)
        }
        
        self.analysis_results['widget_captions'] = analysis
        
        print(f"ì´ ìº¡ì…˜: {analysis['total_captions']}ê°œ")
        print(f"ê³ ìœ  ìº¡ì…˜: {analysis['unique_captions']}ê°œ")
        print(f"ì•¡ì…˜ ë‹¨ì–´: {len(analysis['action_words'])}ê°œ")
        print(f"íŒ¨í„´: {len(analysis['patterns'])}ê°œ")
        print("âœ… ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„° ë¶„ì„ ì™„ë£Œ\n")
    
    def _extract_caption_patterns(self, captions: List[str]) -> Dict[str, List[str]]:
        """ìº¡ì…˜ì—ì„œ íŒ¨í„´ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        patterns = defaultdict(list)
        
        # ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤
        pattern_keywords = {
            'navigation': ['go', 'back', 'forward', 'next', 'previous'],
            'action': ['click', 'tap', 'press', 'select', 'choose'],
            'input': ['enter', 'type', 'input', 'write'],
            'search': ['search', 'find', 'look', 'browse'],
            'media': ['camera', 'photo', 'video', 'image', 'picture'],
            'communication': ['call', 'message', 'email', 'chat', 'send'],
            'social': ['share', 'like', 'favorite', 'bookmark', 'follow'],
            'settings': ['settings', 'config', 'options', 'preferences'],
            'creation': ['add', 'create', 'new', 'make'],
            'modification': ['edit', 'modify', 'change', 'update'],
            'deletion': ['delete', 'remove', 'clear', 'erase'],
            'confirmation': ['confirm', 'save', 'submit', 'ok'],
            'cancellation': ['cancel', 'close', 'dismiss', 'exit']
        }
        
        for caption in captions:
            caption_lower = caption.lower()
            for category, keywords in pattern_keywords.items():
                for keyword in keywords:
                    if keyword in caption_lower:
                        patterns[category].append(caption)
                        break
        
        return dict(patterns)
    
    def generate_comprehensive_rules(self):
        """ì¢…í•©ì ì¸ ê·œì¹™ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        print("=== ì¢…í•© ê·œì¹™ ìƒì„± ===")
        
        rules = {
            'resource_id_rules': {},
            'class_name_rules': {},
            'text_pattern_rules': {},
            'app_specific_rules': {},
            'context_rules': {},
            'action_rules': {}
        }
        
        # 1. ë¦¬ì†ŒìŠ¤ ID ê·œì¹™ ìƒì„±
        if 'processed_data' in self.analysis_results:
            context_analysis = self.analysis_results['processed_data']['context_analysis']
            caption_analysis = self.analysis_results['processed_data']['caption_analysis']
            
            # ë¦¬ì†ŒìŠ¤ IDì™€ ìº¡ì…˜ ë§¤í•‘
            resource_caption_map = defaultdict(list)
            for _, row in self.processed_data.iterrows():
                try:
                    context = json.loads(row['icon_context'])
                    resource_id = context.get('UI element info', {}).get('resource_id', '')
                    captions = row['reference_captions'].split('|')
                    
                    if resource_id:
                        resource_caption_map[resource_id].extend(captions)
                except:
                    continue
            
            # ìƒìœ„ ë¹ˆë„ ìº¡ì…˜ ì„ íƒ
            for resource_id, captions in resource_caption_map.items():
                caption_counts = Counter(captions)
                top_captions = [caption for caption, _ in caption_counts.most_common(3)]
                rules['resource_id_rules'][resource_id] = top_captions
        
        # 2. í´ë˜ìŠ¤ëª… ê·œì¹™ ìƒì„±
        class_caption_map = defaultdict(list)
        for _, row in self.processed_data.iterrows():
            try:
                context = json.loads(row['icon_context'])
                class_name = context.get('UI element info', {}).get('class_name', '')
                captions = row['reference_captions'].split('|')
                
                if class_name:
                    class_caption_map[class_name].extend(captions)
            except:
                continue
        
        for class_name, captions in class_caption_map.items():
            caption_counts = Counter(captions)
            top_captions = [caption for caption, _ in caption_counts.most_common(3)]
            rules['class_name_rules'][class_name] = top_captions
        
        # 3. í…ìŠ¤íŠ¸ íŒ¨í„´ ê·œì¹™ ìƒì„±
        if 'widget_captions' in self.analysis_results:
            patterns = self.analysis_results['widget_captions']['patterns']
            for category, captions in patterns.items():
                caption_counts = Counter(captions)
                top_captions = [caption for caption, _ in caption_counts.most_common(3)]
                rules['text_pattern_rules'][category] = top_captions
        
        # 4. ì•±ë³„ ê·œì¹™ ìƒì„±
        if 'processed_data' in self.analysis_results:
            app_analysis = self.analysis_results['processed_data']['app_analysis']
            for app_name, data in app_analysis.items():
                if data['total_elements'] >= 5:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ì•±ë§Œ
                    app_captions = []
                    for _, row in self.processed_data.iterrows():
                        try:
                            context = json.loads(row['icon_context'])
                            if context.get('app activity name', '') == app_name:
                                captions = row['reference_captions'].split('|')
                                app_captions.extend(captions)
                        except:
                            continue
                    
                    if app_captions:
                        caption_counts = Counter(app_captions)
                        top_captions = [caption for caption, _ in caption_counts.most_common(5)]
                        rules['app_specific_rules'][app_name] = top_captions
        
        # 5. ì•¡ì…˜ ê·œì¹™ ìƒì„±
        if 'processed_data' in self.analysis_results:
            action_words = self.analysis_results['processed_data']['caption_analysis']['action_words']
            rules['action_rules'] = action_words
        
        # ê·œì¹™ íŒŒì¼ ì €ì¥
        import os
        os.makedirs('comprehensive_rules', exist_ok=True)
        
        for rule_type, rule_data in rules.items():
            with open(f'comprehensive_rules/{rule_type}.json', 'w', encoding='utf-8') as f:
                json.dump(rule_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ì¢…í•© ê·œì¹™ ìƒì„± ì™„ë£Œ: {len(rules)}ê°œ ê·œì¹™ íƒ€ì…")
        print(f"ğŸ“ ê·œì¹™ íŒŒì¼ ì €ì¥: comprehensive_rules/ ë””ë ‰í† ë¦¬")
        
        return rules
    
    def print_summary(self):
        """ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print("\n" + "="*50)
        print("ğŸ“Š ì¢…í•© ë°ì´í„° ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        
        if 'processed_data' in self.analysis_results:
            pd = self.analysis_results['processed_data']
            print(f"\nğŸ“± ì²˜ë¦¬ëœ ë°ì´í„°:")
            print(f"  â€¢ ì´ ë ˆì½”ë“œ: {len(self.processed_data)}ê°œ")
            print(f"  â€¢ ê³ ìœ  ë¦¬ì†ŒìŠ¤ ID: {pd['context_analysis']['resource_ids']['unique']}ê°œ")
            print(f"  â€¢ ê³ ìœ  í´ë˜ìŠ¤ëª…: {pd['context_analysis']['class_names']['unique']}ê°œ")
            print(f"  â€¢ ê³ ìœ  ì•±: {pd['context_analysis']['app_names']['unique']}ê°œ")
            print(f"  â€¢ ê³ ìœ  ìº¡ì…˜: {pd['caption_analysis']['unique_captions']}ê°œ")
        
        if 'widget_captions' in self.analysis_results:
            wc = self.analysis_results['widget_captions']
            print(f"\nğŸ·ï¸ ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„°:")
            print(f"  â€¢ ì´ ìº¡ì…˜: {wc['total_captions']}ê°œ")
            print(f"  â€¢ ê³ ìœ  ìº¡ì…˜: {wc['unique_captions']}ê°œ")
            print(f"  â€¢ ì•¡ì…˜ ë‹¨ì–´: {len(wc['action_words'])}ê°œ")
            print(f"  â€¢ íŒ¨í„´ ì¹´í…Œê³ ë¦¬: {len(wc['patterns'])}ê°œ")
        
        print("\n" + "="*50)

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    analyzer = ComprehensiveDataAnalyzer()
    
    # 1. ëª¨ë“  ë°ì´í„° ë¡œë“œ
    analyzer.load_all_data()
    
    # 2. ì²˜ë¦¬ëœ ë°ì´í„° ë¶„ì„
    analyzer.analyze_processed_data()
    
    # 3. ìœ„ì ¯ ìº¡ì…˜ ë°ì´í„° ë¶„ì„
    analyzer.analyze_widget_captions()
    
    # 4. ì¢…í•© ê·œì¹™ ìƒì„±
    rules = analyzer.generate_comprehensive_rules()
    
    # 5. ìš”ì•½ ì¶œë ¥
    analyzer.print_summary()
    
    return analyzer, rules

if __name__ == "__main__":
    analyzer, rules = main() 